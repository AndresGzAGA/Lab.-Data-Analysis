{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkslateblue' size=4>\n",
    "    \n",
    "  # Convolutional Neural Network (ConvNet o CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    " \n",
    "> ## Aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "\n",
    "Las principales aplicaciones para las CNN son:\n",
    "\n",
    "- Clasificación de imágenes ---> identifica si hay un objeto en la imagen.\n",
    "\n",
    "<img src='images/clasification.jpeg' style=\"width: 800px;\"/>\n",
    "\n",
    "- Detección de objetos ---> identifica un objeto y su posición. \n",
    "\n",
    "<img src='images/detection.jpeg' style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "- Neural Style Transfer ---> se juntan un par de imágenes: una de contenido y otra de estilo.\n",
    "\n",
    "<img src='images/style.jpeg' style=\"width: 800px;\"/>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    " \n",
    "> ## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>   \n",
    "Las redes neuronales normales podrían funcionar bien con imágenes de pequeño tamaño. Por ejemplo, las imágenes con un tamaño de 32x32x3 (32 pixeles de ancho, 32 pixeles de alto, 3 canales RGB), una sola neurona completamente conectada en una primera capa oculta de una red neuronal regular tendría 32 * 32 * 3 = 3072 pesos y podría ser manejable. Sin embargo, una imagen de gran tamaño, por ejemplo 200x200x3, conduciría a neuronas que tienen 200 * 200 * 3 = 120,000 pesos. Claramente, esta conectividad total es un desperdicio y la gran cantidad de parámetros conduciría rápidamente a sobreajuste.\n",
    "\n",
    "\n",
    "Los requisitos computacionales y los requisitos de memoria para entrenar una red neuronal con miles de millones de parámetros es poco viable. \n",
    "\n",
    "\n",
    "En particular, a diferencia de una red neuronal normal, las capas de una CNN tienen neuronas dispuestas en 3 dimensiones: ancho, alto, profundidad.\n",
    "\n",
    "Una CNN simple es una secuencia de capas, y cada capa de una CNN transforma un volumen de activaciones en otro a través de una función diferenciable. \n",
    "\n",
    "Básicamente, se utilizan tres tipos principales de capas para construir arquitecturas CNN: \n",
    "\n",
    "1. Capa convolucional,\n",
    "\n",
    "\n",
    "2. Capa de agrupación y \n",
    "\n",
    "\n",
    "3. Capa totalmente conectada. \n",
    "\n",
    "Todas estas capas se apilan para formar una arquitectura CNN completa. \n",
    "\n",
    "\n",
    "<font color='rosybrown'>\n",
    "\n",
    "\n",
    "> ## 1. Capa convolucional\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "La capa convolucional usa una serie de filtros para extraer las caracteristicas de los datos de entrada. \n",
    "\n",
    "Se realiza una operacion de convolucion entre los filtros y las entradas. \n",
    "\n",
    "\n",
    "<img src='images/conv.gif' style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "El recuadro inicial corresponde a los datos de entrada, en el caso de una imagen, cada valor correspondería a un valor de pixel. \n",
    "\n",
    "El recuadro de en medio es el filtro o kernel y el último recuadro es el resultado de realizar la operación de convolucion entre la imagen y el filtro.\n",
    "\n",
    "El filtro se suporpone con la imagen iniciando en la esquina superior izquierda, los valores superpuestos se multiplican y finalmente se suman. Por ejemplo, para obtener el primer valor de la convolucion se realizó la siguiente operación:\n",
    "\n",
    "$$0(-1)+0(-2)+75(-1)+0(0)+75(0)+80(0)+0(1)+75(2)+80(1) = 155$$\n",
    "\n",
    "Posteriormente, el filtro se recorre un lugar a la derecha y se realiza el mismo procedimiento. Una vez que el filtro queda en la parte superior derecha, se baja un lugar y comienza de nuevo en la izquierda, como se muestra en la imagen anterior.\n",
    "\n",
    "\n",
    "En el siguiente ejemplo, se ilustra cómo es que un filtro extrae características de las entradas, por ejemplo, si la entrada es una imagen, entonces para detectar rectas verticales en ella se usa el siguiente filtro:\n",
    "\n",
    "\n",
    "<img src='images/edgedect.png' style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Los colores oscuros corresponden a intensidades bajas.\n",
    "\n",
    "\n",
    "<font color='cornflowerblue'>\n",
    "\n",
    "NOTA\n",
    "\n",
    "No se requiere seleccionar los numeros que definen al filtro, tal vez puedan aprenderse y tratar  a los numeros como parámetros (en partícular, pesos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    "    \n",
    ">  ## 1.1 Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "Es una modificacion que comunmente se aplica a la imagen antes de realizar la operación de convolución. \n",
    "\n",
    "\n",
    "<img src='images/dimconv.png' style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Si se tiene una imagen de entrada de tamaño $nxn$ y se aplica la operación de convolución con un filtro de tamaño $fxf$, entonces la imagen resultante sera de tamaño $(n-f+1)x(n-f+1)$.\n",
    "\n",
    "De la operación de convolución tenemos dos observaciones:\n",
    "\n",
    "1. La imagen se encoge rapidamente por lo que la aplicación de la convolución se podría realizar pocas veces.\n",
    "\n",
    "\n",
    "2. Los pixeles de los extremos se usan una sola vez para calcular la salida por lo que se pierde información cerca de los bordes de la imagen. \n",
    "\n",
    "Una manera de resolver los problemas generados por las dos observaciones anteriores es agregar una columna o renglon (según sea el caso) en cada uno de sus extremos de la imagen inicial, cada valor de pixel en esa orilla agregada es cero:\n",
    "\n",
    "<img src='images/padd.png' style=\"width: 700px;\"/>\n",
    "\n",
    "donde p es el numero de renglones o columnas que se agregan.\n",
    "\n",
    "Comunmente, la tecnica de padding se divide en dos tipos:\n",
    "\n",
    "1. Valid\n",
    "\n",
    "Lo que significa que NO se agrega nada a la imagen inicial, es decir, se aplica la operación de convolución sin antes modificar la imagen de entrada, no se realiza padding.\n",
    "\n",
    "2. Same\n",
    "\n",
    "Se agregan un padding tal que el tamaño de salida sea el mismo que el tamaño de entrada:\n",
    "\n",
    "\n",
    "\n",
    "$$n+2p-f+1 = n \\rightarrow p = \\dfrac{f-1}{2}$$\n",
    "\n",
    "Comunmente, el tamaño de los filtros es impar, una de las razones es que si se aplica padding, entonces p es simétrico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font  size=4 color='rosybrown'>    \n",
    "    \n",
    ">  ## 1.2 Strided Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "El paso (s) en que se desplaza el filtro para lo operacion de convolución es elegible, por defecto es uno. \n",
    "\n",
    "A continuación se muestra un ejemplo con s = 2:\n",
    "\n",
    "\n",
    "<img src='images/stri.png' style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "En este caso para calcular las dimensiones del resultado, se usa la siguiente expresión:\n",
    "\n",
    "$$\\lfloor \\dfrac{n+2p-f}{s}+1\\rfloor x \\lfloor\\dfrac{n+2p-f}{s}+1\\rfloor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    "    \n",
    ">  ## Convolutions in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "Si ahora se quiere aplica las convolucion a imágenes a color, es decir, con los canales RGB, entonces el filtro a aplicar debe tener el mismo número de canales (profundidad). \n",
    "\n",
    "En el siguiente ejemplo se tiene una imagen de 6x6x3, el primer numero corresponde al ancho, el segundo al largo y el tercero a la profundidad o número de canales. Así el filtro a aplicar debe cumplir que es de la forma fxfx3. El resultado después de aplicar la convolución es de tamaño 4x4x1. \n",
    "\n",
    "\n",
    "<img src='images/conv3d.png' style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Suponiendo que estamos interesados en detectar las lineas verticales del canal rojo, entonces el filtro que aplicariamos es el correspondiente a la detección de lineas verticales en el canal rojo R y ceros en los otros dos canales: verde y azul, como se muestra a continuación:\n",
    "\n",
    "<img src='images/filtroR.png' style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    "    \n",
    ">  ## Multiple convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "Se puede aplicar mas de un filtro a la vez. Por ejemplo, el primer filtro podría ser para detectar lineas verticales, y podriamos aplicar un segundo filtro para detectar lineas horizontales.\n",
    "\n",
    "En este caso, cada filtro se aplica como en el caso descrito arriba, y tendriamos dos salidas de 4x4, lo que se hace es apilar estas salidas formando un volumen resultante de 4x4x2.\n",
    "\n",
    "\n",
    "<img src='images/multiple.png' style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "El numero de filtros aplicados determina la profundidad de la salida. \n",
    "\n",
    "\n",
    "Resumiendo, si tenemos una entrada con dimensiones $nxnxn_c$ (con $n_c$ el numero de canales) y un filtro de dimensiones $fxfxn_c$, el resultado al aplicar la convolución a los $n_c´$ filtros es $(n-f+1)x(n-f+1)xn_c´$, aquí se esta suponiendo que p=0 y s = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    "    \n",
    ">  ## Convolution as a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "La entrada de una red neuronal, es decir, el conjunto de rasgos que define el numero de nodos en la capa de entrada se denota con $x = a^{[0]}$, en una red neuronal convencional, lo que se hace es la combinación lineal entre $a^{[0]}$ y la matriz de pesos definida entre las capas de entrada y la capa oculta 1: $w^{[1]}$ y el bias asociado a esa capa $b^1$:\n",
    "\n",
    "$$z^{[1]} = w^{[1]}a^{[0]}+b^{[1]}$$\n",
    "\n",
    "\n",
    "y para obtener la salida de la capa 1, es decir, los valores $a^{[1]}$ que entraran a la siguiente capa para hacer un procedimiento análogo al descrito anteriormente, se aplica una función de activación:\n",
    "\n",
    "$$a^{[1]} = f(z^{[1]})$$\n",
    "\n",
    "\n",
    "<img src='images/annf.gif' style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "En la siguiente imagen se muestra la analogía entre el proceso para calcular las salidas de una capa usando una capa convolucional:\n",
    "\n",
    "\n",
    "<img src='images/analogy.png' style=\"width: 700px;\"/>\n",
    "\n",
    "En este ejemplo, la cantidad de parámetros a entrenar es de 2(3x3x3+1), pues se aplican dos filtros de 3x3x3, que juegan el papel de los pesos y sumamos el parámetro del bias. \n",
    "\n",
    "Algo importante a notar es que no importa cuán grande sea la imagen de entrada, la cantidad de parámetros es fija. \n",
    "\n",
    "**1. Ejercicio**\n",
    "\n",
    "Si aplicas 10 filtros de 3x3x3 en una capa de una red neuronal, ¿Cuántos parámetros (pesos y bias) hay en esa capa?\n",
    "\n",
    "**2. Ejercicio**\n",
    "\n",
    "<img src='images/ex1.png' style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Calcula las dimensiones de cada una de las salidas en donde se muestra ???, es decir, comenzando con el volumen de 39x39x3 y aplicar una capa de convolución usando 10 filtros de 3x3x3, un paso de 1 y sin usar padding, ¿qué dimensiones tendria el siguiente volumen? y así sucesivamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    "  ## Ejemplos de aplicación de filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "  <font size=4 color='black'>\n",
    "  \n",
    "  Scipy es una librería para el procesamiento de imágenes, entre otras muchas cosas.\n",
    "  \n",
    "  [scipy](https://www.scipy.org/)\n",
    "  \n",
    "  scipy.misc.ascent(): Get an 8-bit grayscale bit-depth, 512 x 512 derived image for easy use in demos.\n",
    "  \n",
    "  Imagen antes de aplicar el filtro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "i = misc.ascent()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.grid(False)\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "plt.imshow(i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_transformed = np.copy(i)\n",
    "size_x = i_transformed.shape[0]\n",
    "size_y = i_transformed.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    " Definición de filtros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This filter detects edges nicely\n",
    "# It creates a convolution that only passes through sharp edges and straight lines.\n",
    "\n",
    "#Experiment with different values for fun effects.\n",
    "\n",
    "#filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]]\n",
    "\n",
    "# A couple more filters to try for fun!\n",
    "\n",
    "#filter = [ [-1, -2, -1], [0, 0, 0], [1, 2, 1]]\n",
    "#filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\n",
    "#filter = [ [1, 0, -1], [1, 0, -1], [1, 0, -1]]\n",
    "filter = [ [1, 1, 1], [0, 0, 0], [-1, -1, -1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "Operación de convolución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,size_x-1):\n",
    "    for y in range(1,size_y-1):\n",
    "        convolution = 0.0\n",
    "        convolution = convolution + (i[x - 1, y-1] * filter[0][0])\n",
    "        convolution = convolution + (i[x, y-1] * filter[0][1])\n",
    "        convolution = convolution + (i[x + 1, y-1] * filter[0][2])\n",
    "        convolution = convolution + (i[x-1, y] * filter[1][0])\n",
    "        convolution = convolution + (i[x, y] * filter[1][1])\n",
    "        convolution = convolution + (i[x+1, y] * filter[1][2])\n",
    "        convolution = convolution + (i[x-1, y+1] * filter[2][0])\n",
    "        convolution = convolution + (i[x, y+1] * filter[2][1])\n",
    "        convolution = convolution + (i[x+1, y+1] * filter[2][2])\n",
    "        convolution = convolution \n",
    "        if(convolution<0):\n",
    "            convolution=0\n",
    "        if(convolution>255):\n",
    "            convolution=255\n",
    "        i_transformed[x, y] = convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "  Imagen después de aplicar el filtro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "plt.grid(False)\n",
    "plt.imshow(i_transformed)\n",
    "plt.axis('on')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "  **3. Ejercicio**\n",
    "  \n",
    "Aplicar algún otro filtro a la imagen anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4  color='rosybrown'>\n",
    "\n",
    "> ## 2. Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "\n",
    "Para reducir las dimensiones de la entrada se usan las capas de agrupación, para ello se obtiene el promedio o máximo de un conjunto de valores.\n",
    "\n",
    "<img src='images/maxpool.gif' style=\"width: 400px;\"/>\n",
    "\n",
    "Una propiedad interesante del máximo agrupamiento es que tiene un conjunto de hiperparámetros, pero no tiene parámetros que aprender.\n",
    "\n",
    "Una vez que fijes f y s, es sólo un cálculo fijo y el descenso de gradiente no cambia nada.\n",
    "\n",
    "Otro tipo de capa de agrupamiento es average pooling, que en lugar de calcular el maximo calcula el promedio. \n",
    "\n",
    "\n",
    "Si tiene una entrada 3D, entonces las salidas tendrán la misma dimensión, la forma en que se calcula\n",
    "el máximo agrupamiento es realizar el cálculo que acabamos de describir en cada uno de los canales de forma independiente. El cálculo del máximo agrupamiento se realiza independientemente en cada uno de estos $n_c$ canales.\n",
    "\n",
    "Resumen: en este tipo de capa no hay parametros para aprender, solo se fijan los hiperparametros: s, f y el tipo de capa de agrupamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='rosybrown'>\n",
    "    \n",
    " > ## Fully Connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "Finalmente se tiene la capa fully-connected (FC), que es como las que se han trabajado hasta ahora, un conjunto de nodos que estan totalmente conectados con los nodos de la capa anterior.\n",
    "\n",
    "\n",
    "**Ejemplo de una CNN**\n",
    "\n",
    "<img src='images/example.png' style=\"width: 900px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "**4. Ejercicio**\n",
    "\n",
    "Completa la siguiente tabla:\n",
    "\n",
    "| Layer | Activation shape | Activation size | # parameters |\n",
    "| --- | --- | --- | --- |\n",
    "| input | (32, 32, 3) | 3072 | 0 |\n",
    "| conv1(f=5, s=1, 8filters) | --- | --- | --- |\n",
    "| pool1(f=2, s=2) | --- | --- | --- |\n",
    "| conv2(f=5, s=1, 16filters) | --- | --- | --- |\n",
    "| pool2(f=2, s=2) | --- | --- | --- |\n",
    "| FC3 | (120,1) | --- | --- |\n",
    "| FC4 | (84,1) | --- | --- |\n",
    "| Softmax | (10, 1) | --- | --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    " A continuación se dejan algunas referencias interesantes:\n",
    " \n",
    " CNN explicaciones interactivas:\n",
    "\n",
    "[CNN explainer](https://poloclub.github.io/cnn-explainer/)\n",
    "\n",
    "[Artículo de CNN explainer](https://arxiv.org/pdf/2004.15004.pdf)\n",
    "\n",
    "3d visualization of CNN\n",
    "\n",
    "[An Interactive Node-Link Visualization of Convolutional Neural Networks](https://www.cs.ryerson.ca/~aharley/vis/conv/)\n",
    "\n",
    "[Artículo An Interactive Node-Link Visualization of Convolutional Neural Networks](https://www.cs.ryerson.ca/~aharley/vis/)\n",
    "\n",
    "Convnet playground\n",
    "\n",
    "[Convnet playground 1](https://towardsdatascience.com/convnetplayground-979d441ebf82)\n",
    "\n",
    "[Convnet playground](https://convnetplayground.fastforwardlabs.com/#/)\n",
    "\n",
    " ver las ‘imágenes’ después de aplicar filtros:\n",
    " \n",
    "[Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "\n",
    "Librería de Python para ver las ‘imágenes’ después de aplicar filtros:\n",
    "\n",
    "[cnn-visualizer](https://github.com/penny4860/cnn-visualizer)\n",
    "\n",
    "[Picasso](https://github.com/merantix/picasso)\n",
    "\n",
    "[CNN Visualizer: Toolkit for Visualizing Units in Deep Convolutional Neural Networks](https://github.com/zhoubolei/cnnvisualizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
